{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PhuongUyen-tt/blank-app/blob/main/B%E1%BA%A3n_sao_c%E1%BB%A7a_Untitled3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nzkgRNYJOp9f",
        "outputId": "417dbaa9-04c2-4a74-d6de-0f29fdf8893c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# prompt: kết nối với gg drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: đọc file theo đường dẫn: /content/drive/MyDrive/HK II | 24.25/AI in Biz /BTL /data.csv\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "file_path = '/content/drive/MyDrive/HK II | 24.25/AI in Biz /BTL /data.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S97LO2Zek4gg",
        "outputId": "e5ee2bbe-090a-40bf-d878-633fe7181417"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   transaction_id     transaction_date transaction_time  store_id  \\\n",
            "0            1567  2023-03-19 00:00:00         20:30:42       501   \n",
            "1            1568  2022-07-14 00:00:00         22:59:16       302   \n",
            "2            1569  2023-05-03 00:00:00         15:31:33       201   \n",
            "3            1570  2025-04-11 00:00:00         15:30:54       202   \n",
            "4            1571  2024-02-20 00:00:00         09:25:54       401   \n",
            "\n",
            "            store_location  product_id  transaction_qty  unit_price  \\\n",
            "0  Nha Trang - City Center           7                2       45000   \n",
            "1      Da Nang - Thanh Khe           1                1       29000   \n",
            "2        Hanoi - Hoan Kiem          24                4       35000   \n",
            "3          Hanoi - Ba Dinh          15                5       55000   \n",
            "4        Hue - City Center           3                2       29000   \n",
            "\n",
            "   Total_Bill    product_category product_type    product_detail     Size  \\\n",
            "0       90000                 Tea          Tea      Tra Sen Vang        M   \n",
            "1       29000  Traditional Coffee       Coffee       Phin Sua Da        M   \n",
            "2      140000                Food      Banh Mi    Banh Mi Ga Que  Regular   \n",
            "3      275000              Freeze       Freeze  Freeze Chocolate        M   \n",
            "4       58000  Traditional Coffee       Coffee       Phin Den Da        M   \n",
            "\n",
            "   customer_id   customer_name  Gender  Age           Occupation Season  \\\n",
            "0         9143     Lý Anh Xuân    Male   68           Nghề tự do   Xuân   \n",
            "1         1935    Mai Thanh An  Female   80           Nghề tự do     Hè   \n",
            "2         7189          Lê Yến    Male   24  Sinh viên, học sinh   Xuân   \n",
            "3          246     Lý Thị Xuân    Male   22  Sinh viên, học sinh   Xuân   \n",
            "4         8704  Bùi Anh Phương  Female   23  Sinh viên, học sinh   Đông   \n",
            "\n",
            "    Income  \n",
            "0  7000000  \n",
            "1  7000000  \n",
            "2  3000000  \n",
            "3  3000000  \n",
            "4  3000000  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: chia tập dữ liệu thành 2 tập train và tập test với test sz 0.2\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assuming df is your DataFrame\n",
        "# X will be the features (all columns except 'target_column')\n",
        "# y will be the target variable ('target_column')\n",
        "# Replace 'target_column' with the actual name of your target column\n",
        "# If your data doesn't have a specific target column and you are splitting for unsupervised learning,\n",
        "# you might just split the entire dataframe X = df\n",
        "if 'target_column' in df.columns:\n",
        "  X = df.drop('target_column', axis=1)\n",
        "  y = df['target_column']\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "  print(\"\\nData split into training and testing sets:\")\n",
        "  print(\"X_train shape:\", X_train.shape)\n",
        "  print(\"X_test shape:\", X_test.shape)\n",
        "  print(\"y_train shape:\", y_train.shape)\n",
        "  print(\"y_test shape:\", y_test.shape)\n",
        "else:\n",
        "  print(\"\\nAssuming unsupervised splitting (no target column specified).\")\n",
        "  X_train, X_test = train_test_split(df, test_size=0.2, random_state=42)\n",
        "  print(\"\\nData split into training and testing sets:\")\n",
        "  print(\"X_train shape:\", X_train.shape)\n",
        "  print(\"X_test shape:\", X_test.shape)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQ4WKRoAlIoy",
        "outputId": "6b1523c4-1bfd-4546-833a-4ca1870e75fc"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Assuming unsupervised splitting (no target column specified).\n",
            "\n",
            "Data split into training and testing sets:\n",
            "X_train shape: (24000, 20)\n",
            "X_test shape: (6000, 20)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Preprocess data, train a model to predict \"product_category\" from key features \" store_location\".,\"Gender\", \"Age'\", \"Total_Bill\", \"Season\", \"Size\", \"transaction_qty\" with Gradient Boosting Classification model\n",
        "\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "# Define features and target\n",
        "features = [\"store_location\", \"Gender\", \"Age\", \"Total_Bill\", \"Season\", \"Size\", \"transaction_qty\"]\n",
        "target = \"product_category\"\n",
        "\n",
        "# Check if the target column exists\n",
        "if target not in df.columns:\n",
        "    print(f\"Error: Target column '{target}' not found in the DataFrame.\")\n",
        "else:\n",
        "    # Check if all features exist\n",
        "    missing_features = [feat for feat in features if feat not in df.columns]\n",
        "    if missing_features:\n",
        "        print(f\"Error: Missing feature columns: {missing_features}\")\n",
        "    else:\n",
        "        # Preprocessing: Handle categorical features\n",
        "        X = df[features].copy()\n",
        "        y = df[target].copy()\n",
        "\n",
        "        # Use Label Encoding for simplicity on categorical features\n",
        "        categorical_features = [\"store_location\", \"Gender\", \"Season\", \"Size\"]\n",
        "        for col in categorical_features:\n",
        "            if col in X.columns:\n",
        "                le = LabelEncoder()\n",
        "                X[col] = le.fit_transform(X[col])\n",
        "\n",
        "        # Split data\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "        # Train Gradient Boosting Classifier\n",
        "        gb_model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "        gb_model.fit(X_train, y_train)\n",
        "\n",
        "        # Predict on the test set\n",
        "        y_pred = gb_model.predict(X_test)\n",
        "\n",
        "        # Evaluate the model\n",
        "        print(\"\\nGradient Boosting Classifier Results:\")\n",
        "        print(\"Classification Report:\")\n",
        "        print(classification_report(y_test, y_pred))\n",
        "        print(\"\\nConfusion Matrix:\")\n",
        "        print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "        # Optional: Print feature importances\n",
        "        print(\"\\nFeature Importances:\")\n",
        "        feature_importances = pd.Series(gb_model.feature_importances_, index=features).sort_values(ascending=False)\n",
        "feature_importances"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 720
        },
        "id": "DLPJbX3Dse0h",
        "outputId": "89db0169-a4d7-4792-d55f-da4a2df856e4"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Gradient Boosting Classifier Results:\n",
            "Classification Report:\n",
            "                       precision    recall  f1-score   support\n",
            "\n",
            "Espresso Based Coffee       0.57      0.48      0.52      1381\n",
            "                 Food       1.00      1.00      1.00      1396\n",
            "               Freeze       0.70      0.58      0.64      1199\n",
            "                  Tea       0.66      1.00      0.79       813\n",
            "   Traditional Coffee       1.00      1.00      1.00      1211\n",
            "\n",
            "             accuracy                           0.80      6000\n",
            "            macro avg       0.79      0.81      0.79      6000\n",
            "         weighted avg       0.80      0.80      0.79      6000\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[ 667    0  290  424    0]\n",
            " [   0 1396    0    0    0]\n",
            " [ 506    0  693    0    0]\n",
            " [   0    0    0  813    0]\n",
            " [   0    0    0    0 1211]]\n",
            "\n",
            "Feature Importances:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Size               0.487382\n",
              "Total_Bill         0.374039\n",
              "transaction_qty    0.136607\n",
              "Age                0.000798\n",
              "store_location     0.000587\n",
              "Season             0.000383\n",
              "Gender             0.000206\n",
              "dtype: float64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Size</th>\n",
              "      <td>0.487382</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Total_Bill</th>\n",
              "      <td>0.374039</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>transaction_qty</th>\n",
              "      <td>0.136607</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Age</th>\n",
              "      <td>0.000798</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>store_location</th>\n",
              "      <td>0.000587</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Season</th>\n",
              "      <td>0.000383</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Gender</th>\n",
              "      <td>0.000206</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> float64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
        "import xgboost as xgb\n",
        "from scipy.stats import uniform, randint\n",
        "from sklearn.preprocessing import LabelEncoder # Import LabelEncoder\n",
        "\n",
        "# Select a subset of potentially relevant columns for feature engineering\n",
        "selected_cols = [\"store_location\", \"Gender\", \"Age\", \"Total_Bill\", \"Season\", \"Size\", \"transaction_qty\"]\n",
        "# Assuming df is already loaded from previous cells\n",
        "# df = pd.read_csv(file_path) # If df is not available, uncomment and load your data here\n",
        "df_subset = df[selected_cols].copy()\n",
        "\n",
        "# Identify categorical columns in the subset and apply one-hot encoding\n",
        "categorical_cols_subset = df_subset.select_dtypes(include=['object']).columns\n",
        "df_subset = pd.get_dummies(df_subset, columns=categorical_cols_subset, drop_first=True)\n",
        "\n",
        "# Define features (X) using the subset DataFrame\n",
        "X = df_subset\n",
        "\n",
        "# Define and encode the target variable (y)\n",
        "y = df['product_category'] # Use the original 'product_category' as the target\n",
        "\n",
        "# Use LabelEncoder to convert the target variable (product_category) into numerical labels\n",
        "label_encoder_y = LabelEncoder()\n",
        "y_encoded = label_encoder_y.fit_transform(y)\n",
        "\n",
        "# Split the data into training and testing sets using the encoded target\n",
        "X_train, X_test, y_train_encoded, y_test_encoded = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Instantiate the XGBoost Regressor model\n",
        "# Since you are predicting a categorical target, you should use XGBClassifier instead of XGBRegressor\n",
        "xgb_model = xgb.XGBClassifier(random_state=42) # Changed to XGBClassifier\n",
        "\n",
        "# Define a smaller parameter distribution for RandomizedSearchCV\n",
        "# Adjust parameters for a classifier if needed, but the current range might be okay\n",
        "param_dist = {\n",
        "    'n_estimators': randint(50, 200),\n",
        "    'learning_rate': uniform(0.01, 0.05),\n",
        "    'max_depth': randint(3, 7),\n",
        "    'min_child_weight': randint(1, 4),\n",
        "    'subsample': uniform(0.8, 0.2),\n",
        "    'colsample_bytree': uniform(0.8, 0.2)\n",
        "}\n",
        "\n",
        "# Instantiate RandomizedSearchCV with reduced cv and n_jobs=1\n",
        "# Change scoring to 'accuracy' or other classification metrics\n",
        "random_search = RandomizedSearchCV(estimator=xgb_model, param_distributions=param_dist,\n",
        "                                   n_iter=30, scoring='accuracy', cv=2, n_jobs=1, verbose=2, random_state=42) # Changed scoring to 'accuracy'\n",
        "\n",
        "# Fit RandomizedSearchCV to the training data using the encoded target\n",
        "print(\"Performing Randomized Search for Hyperparameter Tuning...\")\n",
        "random_search.fit(X_train, y_train_encoded) # Use y_train_encoded\n",
        "\n",
        "# Print the best hyperparameters and best score\n",
        "best_params = random_search.best_params_\n",
        "best_score = random_search.best_score_\n",
        "\n",
        "print(\"\\nBest hyperparameters found: \", best_params)\n",
        "print(\"\\nBest Cross-Validation Accuracy:\") # Changed label for scoring\n",
        "print(best_score)\n",
        "\n",
        "# You might want to evaluate the best model on the test set using appropriate classification metrics\n",
        "# For example:\n",
        "# best_xgb_model = random_search.best_estimator_\n",
        "# y_pred_encoded = best_xgb_model.predict(X_test)\n",
        "# from sklearn.metrics import accuracy_score\n",
        "# test_accuracy = accuracy_score(y_test_encoded, y_pred_encoded)\n",
        "# print(\"\\nTest Accuracy with Best Parameters:\", test_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ym-IxvsS5FX",
        "outputId": "c4a63804-2f24-4d6b-cf86-9821c2d0f8f5"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing Randomized Search for Hyperparameter Tuning...\n",
            "Fitting 2 folds for each of 30 candidates, totalling 60 fits\n",
            "[CV] END colsample_bytree=0.8749080237694725, learning_rate=0.05753571532049581, max_depth=5, min_child_weight=1, n_estimators=70, subsample=0.8312037280884873; total time=   0.6s\n",
            "[CV] END colsample_bytree=0.8749080237694725, learning_rate=0.05753571532049581, max_depth=5, min_child_weight=1, n_estimators=70, subsample=0.8312037280884873; total time=   3.2s\n",
            "[CV] END colsample_bytree=0.8311989040672406, learning_rate=0.012904180608409973, max_depth=6, min_child_weight=1, n_estimators=149, subsample=0.8285733635843882; total time=   5.9s\n",
            "[CV] END colsample_bytree=0.8311989040672406, learning_rate=0.012904180608409973, max_depth=6, min_child_weight=1, n_estimators=149, subsample=0.8285733635843882; total time=   1.3s\n",
            "[CV] END colsample_bytree=0.9301776945897706, learning_rate=0.012820578951355013, max_depth=6, min_child_weight=2, n_estimators=87, subsample=0.8001557531682029; total time=   0.8s\n",
            "[CV] END colsample_bytree=0.9301776945897706, learning_rate=0.012820578951355013, max_depth=6, min_child_weight=2, n_estimators=87, subsample=0.8001557531682029; total time=   0.8s\n",
            "[CV] END colsample_bytree=0.9984423118582435, learning_rate=0.04087407548138583, max_depth=4, min_child_weight=2, n_estimators=138, subsample=0.8582458280396085; total time=   1.0s\n",
            "[CV] END colsample_bytree=0.9984423118582435, learning_rate=0.04087407548138583, max_depth=4, min_child_weight=2, n_estimators=138, subsample=0.8582458280396085; total time=   0.9s\n",
            "[CV] END colsample_bytree=0.9223705789444759, learning_rate=0.016974693032602094, max_depth=6, min_child_weight=3, n_estimators=100, subsample=0.8764923982534326; total time=   0.9s\n",
            "[CV] END colsample_bytree=0.9223705789444759, learning_rate=0.016974693032602094, max_depth=6, min_child_weight=3, n_estimators=100, subsample=0.8764923982534326; total time=   0.9s\n",
            "[CV] END colsample_bytree=0.9966461771613577, learning_rate=0.033338144662399, max_depth=3, min_child_weight=3, n_estimators=184, subsample=0.8341048247374584; total time=   1.2s\n",
            "[CV] END colsample_bytree=0.9966461771613577, learning_rate=0.033338144662399, max_depth=3, min_child_weight=3, n_estimators=184, subsample=0.8341048247374584; total time=   1.3s\n",
            "[CV] END colsample_bytree=0.813010318597056, learning_rate=0.05744427686266667, max_depth=6, min_child_weight=2, n_estimators=58, subsample=0.8031932504440429; total time=   0.8s\n",
            "[CV] END colsample_bytree=0.813010318597056, learning_rate=0.05744427686266667, max_depth=6, min_child_weight=2, n_estimators=58, subsample=0.8031932504440429; total time=   2.7s\n",
            "[CV] END colsample_bytree=0.8461787651244298, learning_rate=0.022051273301300585, max_depth=6, min_child_weight=3, n_estimators=57, subsample=0.8068777042230437; total time=   2.2s\n",
            "[CV] END colsample_bytree=0.8461787651244298, learning_rate=0.022051273301300585, max_depth=6, min_child_weight=3, n_estimators=57, subsample=0.8068777042230437; total time=   0.8s\n",
            "[CV] END colsample_bytree=0.9818640804157565, learning_rate=0.022938999080000848, max_depth=6, min_child_weight=2, n_estimators=183, subsample=0.8415883325736379; total time=   1.6s\n",
            "[CV] END colsample_bytree=0.9818640804157565, learning_rate=0.022938999080000848, max_depth=6, min_child_weight=2, n_estimators=183, subsample=0.8415883325736379; total time=   1.6s\n",
            "[CV] END colsample_bytree=0.9135400655639984, learning_rate=0.01156566462277793, max_depth=4, min_child_weight=2, n_estimators=93, subsample=0.9878997883128379; total time=   0.7s\n",
            "[CV] END colsample_bytree=0.9135400655639984, learning_rate=0.01156566462277793, max_depth=4, min_child_weight=2, n_estimators=93, subsample=0.9878997883128379; total time=   0.7s\n",
            "[CV] END colsample_bytree=0.9789654700855298, learning_rate=0.03989499894055426, max_depth=6, min_child_weight=3, n_estimators=89, subsample=0.9689067697356304; total time=   0.8s\n",
            "[CV] END colsample_bytree=0.9789654700855298, learning_rate=0.03989499894055426, max_depth=6, min_child_weight=3, n_estimators=89, subsample=0.9689067697356304; total time=   0.8s\n",
            "[CV] END colsample_bytree=0.9494640220274763, learning_rate=0.03698460661945399, max_depth=6, min_child_weight=2, n_estimators=173, subsample=0.8551998364045087; total time=   1.4s\n",
            "[CV] END colsample_bytree=0.9494640220274763, learning_rate=0.03698460661945399, max_depth=6, min_child_weight=2, n_estimators=173, subsample=0.8551998364045087; total time=   4.3s\n",
            "[CV] END colsample_bytree=0.8592547011408165, learning_rate=0.018263346953150125, max_depth=3, min_child_weight=3, n_estimators=58, subsample=0.9544489538593315; total time=   0.7s\n",
            "[CV] END colsample_bytree=0.8592547011408165, learning_rate=0.018263346953150125, max_depth=3, min_child_weight=3, n_estimators=58, subsample=0.9544489538593315; total time=   0.4s\n",
            "[CV] END colsample_bytree=0.8397431363068345, learning_rate=0.01027610585618012, max_depth=5, min_child_weight=3, n_estimators=130, subsample=0.94226839054973; total time=   1.0s\n",
            "[CV] END colsample_bytree=0.8397431363068345, learning_rate=0.01027610585618012, max_depth=5, min_child_weight=3, n_estimators=130, subsample=0.94226839054973; total time=   1.0s\n",
            "[CV] END colsample_bytree=0.9580351081062412, learning_rate=0.04029799873905057, max_depth=4, min_child_weight=3, n_estimators=90, subsample=0.9829919351087562; total time=   0.7s\n",
            "[CV] END colsample_bytree=0.9580351081062412, learning_rate=0.04029799873905057, max_depth=4, min_child_weight=3, n_estimators=90, subsample=0.9829919351087562; total time=   0.7s\n",
            "[CV] END colsample_bytree=0.97000771555796, learning_rate=0.032472533706910174, max_depth=3, min_child_weight=3, n_estimators=111, subsample=0.8650366644053494; total time=   0.7s\n",
            "[CV] END colsample_bytree=0.97000771555796, learning_rate=0.032472533706910174, max_depth=3, min_child_weight=3, n_estimators=111, subsample=0.8650366644053494; total time=   0.7s\n",
            "[CV] END colsample_bytree=0.9459212356676129, learning_rate=0.04187787356776066, max_depth=5, min_child_weight=3, n_estimators=150, subsample=0.9943424190778208; total time=   1.1s\n",
            "[CV] END colsample_bytree=0.9459212356676129, learning_rate=0.04187787356776066, max_depth=5, min_child_weight=3, n_estimators=150, subsample=0.9943424190778208; total time=   1.2s\n",
            "[CV] END colsample_bytree=0.9697827648532168, learning_rate=0.04608647605824366, max_depth=5, min_child_weight=3, n_estimators=191, subsample=0.8987591192728782; total time=   1.5s\n",
            "[CV] END colsample_bytree=0.9697827648532168, learning_rate=0.04608647605824366, max_depth=5, min_child_weight=3, n_estimators=191, subsample=0.8987591192728782; total time=   4.0s\n",
            "[CV] END colsample_bytree=0.9045465658763989, learning_rate=0.03137705091792748, max_depth=4, min_child_weight=1, n_estimators=112, subsample=0.9791527191347039; total time=   1.1s\n",
            "[CV] END colsample_bytree=0.9045465658763989, learning_rate=0.03137705091792748, max_depth=4, min_child_weight=1, n_estimators=112, subsample=0.9791527191347039; total time=   0.8s\n",
            "[CV] END colsample_bytree=0.8950740446364224, learning_rate=0.038163778598819184, max_depth=6, min_child_weight=2, n_estimators=192, subsample=0.9511102277086098; total time=   1.7s\n",
            "[CV] END colsample_bytree=0.8950740446364224, learning_rate=0.038163778598819184, max_depth=6, min_child_weight=2, n_estimators=192, subsample=0.9511102277086098; total time=   1.6s\n",
            "[CV] END colsample_bytree=0.8457596330983246, learning_rate=0.01384899549143965, max_depth=5, min_child_weight=3, n_estimators=135, subsample=0.9760935678030516; total time=   1.0s\n",
            "[CV] END colsample_bytree=0.8457596330983246, learning_rate=0.01384899549143965, max_depth=5, min_child_weight=3, n_estimators=135, subsample=0.9760935678030516; total time=   1.1s\n",
            "[CV] END colsample_bytree=0.9248708096267587, learning_rate=0.0247816842918857, max_depth=3, min_child_weight=2, n_estimators=77, subsample=0.960734415379823; total time=   0.6s\n",
            "[CV] END colsample_bytree=0.9248708096267587, learning_rate=0.0247816842918857, max_depth=3, min_child_weight=2, n_estimators=77, subsample=0.960734415379823; total time=   0.5s\n",
            "[CV] END colsample_bytree=0.8373140117772072, learning_rate=0.054627949924498896, max_depth=5, min_child_weight=2, n_estimators=141, subsample=0.871259567615395; total time=   1.1s\n",
            "[CV] END colsample_bytree=0.8373140117772072, learning_rate=0.054627949924498896, max_depth=5, min_child_weight=2, n_estimators=141, subsample=0.871259567615395; total time=   1.2s\n",
            "[CV] END colsample_bytree=0.9813656883091508, learning_rate=0.023606612469231767, max_depth=5, min_child_weight=2, n_estimators=170, subsample=0.9636029531844987; total time=   4.2s\n",
            "[CV] END colsample_bytree=0.9813656883091508, learning_rate=0.023606612469231767, max_depth=5, min_child_weight=2, n_estimators=170, subsample=0.9636029531844987; total time=   1.3s\n",
            "[CV] END colsample_bytree=0.9721461166512687, learning_rate=0.010347606526559536, max_depth=6, min_child_weight=3, n_estimators=186, subsample=0.8969659942717967; total time=   1.5s\n",
            "[CV] END colsample_bytree=0.9721461166512687, learning_rate=0.010347606526559536, max_depth=6, min_child_weight=3, n_estimators=186, subsample=0.8969659942717967; total time=   1.5s\n",
            "[CV] END colsample_bytree=0.9384872065780541, learning_rate=0.023470616689926074, max_depth=6, min_child_weight=3, n_estimators=108, subsample=0.8646405864041511; total time=   0.9s\n",
            "[CV] END colsample_bytree=0.9384872065780541, learning_rate=0.023470616689926074, max_depth=6, min_child_weight=3, n_estimators=108, subsample=0.8646405864041511; total time=   0.9s\n",
            "[CV] END colsample_bytree=0.9037581243486733, learning_rate=0.04515094794475889, max_depth=3, min_child_weight=2, n_estimators=101, subsample=0.8493752125677203; total time=   0.7s\n",
            "[CV] END colsample_bytree=0.9037581243486733, learning_rate=0.04515094794475889, max_depth=3, min_child_weight=2, n_estimators=101, subsample=0.8493752125677203; total time=   0.6s\n",
            "[CV] END colsample_bytree=0.9392608545679577, learning_rate=0.045613529496222105, max_depth=3, min_child_weight=1, n_estimators=130, subsample=0.8533562028550571; total time=   0.8s\n",
            "[CV] END colsample_bytree=0.9392608545679577, learning_rate=0.045613529496222105, max_depth=3, min_child_weight=1, n_estimators=130, subsample=0.8533562028550571; total time=   0.8s\n",
            "[CV] END colsample_bytree=0.9953229911665307, learning_rate=0.030551850665911567, max_depth=5, min_child_weight=1, n_estimators=178, subsample=0.8479123781333945; total time=   5.1s\n",
            "[CV] END colsample_bytree=0.9953229911665307, learning_rate=0.030551850665911567, max_depth=5, min_child_weight=1, n_estimators=178, subsample=0.8479123781333945; total time=   1.4s\n",
            "[CV] END colsample_bytree=0.8289789744182446, learning_rate=0.034472638013878155, max_depth=6, min_child_weight=2, n_estimators=117, subsample=0.8161706652665431; total time=   1.0s\n",
            "[CV] END colsample_bytree=0.8289789744182446, learning_rate=0.034472638013878155, max_depth=6, min_child_weight=2, n_estimators=117, subsample=0.8161706652665431; total time=   1.0s\n",
            "\n",
            "Best hyperparameters found:  {'colsample_bytree': np.float64(0.9697827648532168), 'learning_rate': np.float64(0.04608647605824366), 'max_depth': 5, 'min_child_weight': 3, 'n_estimators': 191, 'subsample': np.float64(0.8987591192728782)}\n",
            "\n",
            "Best Cross-Validation Accuracy:\n",
            "0.7997083333333332\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccd55b95"
      },
      "source": [
        "## Streamlit app development\n",
        "\n",
        "### Subtask:\n",
        "Build a Streamlit application that takes seasonal data as input and uses the trained model to predict product demand.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ee7de653"
      },
      "source": [
        "**Reasoning**:\n",
        "Create the `app.py` file with the necessary imports, load the best model, and set up the basic Streamlit app structure with input widgets.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "072aeb64",
        "outputId": "2bf9a2b6-44ea-4e6f-d008-f070fa70e544"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import joblib\n",
        "import numpy as np\n",
        "import os # Import os for checking file existence\n",
        "\n",
        "# Load the trained XGBoost model\n",
        "model_path = 'xgboost_model.pkl'\n",
        "if not os.path.exists(model_path):\n",
        "    st.error(f\"Model file '{model_path}' not found. Please train and save the model first.\")\n",
        "    st.stop()\n",
        "try:\n",
        "    model = joblib.load(model_path)\n",
        "except Exception as e:\n",
        "    st.error(f\"Error loading the model file: {e}\")\n",
        "    st.stop()\n",
        "\n",
        "# Load the feature columns used during training\n",
        "features_path = 'xgboost_features.pkl'\n",
        "if not os.path.exists(features_path):\n",
        "    st.error(f\"Feature columns file '{features_path}' not found. Please run the training steps first to save X_train columns.\")\n",
        "    st.stop()\n",
        "try:\n",
        "    expected_cols = joblib.load(features_path)\n",
        "except Exception as e:\n",
        "    st.error(f\"Error loading the feature columns file: {e}\")\n",
        "    st.stop()\n",
        "\n",
        "# Load the original dataframe to get unique values for dropdowns\n",
        "# This is needed to populate selectboxes with actual data values\n",
        "file_path = '/content/drive/MyDrive/Phú Quốc/du_lieu_da_them_season_va_income( dữ liệu cuối cùng).xlsx - Sheet1.csv'\n",
        "try:\n",
        "    original_df = pd.read_csv(file_path)\n",
        "    unique_store_ids = sorted(original_df['store_id'].unique().tolist())\n",
        "    unique_store_locations = sorted(original_df['store_location'].unique().tolist())\n",
        "    unique_product_ids = sorted(original_df['product_id'].unique().tolist())\n",
        "    unique_occupations = sorted(original_df['Occupation'].unique().tolist())\n",
        "    unique_seasons = sorted(original_df['Season'].unique().tolist())\n",
        "    unique_genders = sorted(original_df['Gender'].unique().tolist())\n",
        "\n",
        "except FileNotFoundError:\n",
        "    st.error(f\"Original data file not found at {file_path}. Please ensure the file exists.\")\n",
        "    st.stop()\n",
        "except KeyError as e:\n",
        "    st.error(f\"Missing expected column in original data file: {e}\")\n",
        "    st.stop()\n",
        "except Exception as e:\n",
        "    st.error(f\"Error loading original data: {e}\")\n",
        "    st.stop()\n",
        "\n",
        "\n",
        "st.title(\"Seasonal Product Demand Forecasting\")\n",
        "st.header(\"Input the details to predict product demand for a season.\")\n",
        "\n",
        "# Define input widgets based on the features used for training (X_train)\n",
        "# Features used in X_train were derived from:\n",
        "# 'store_id', 'product_id', 'transaction_qty', 'unit_price', 'Age', 'Income',\n",
        "# 'Season', 'Occupation', 'month', 'year'\n",
        "# plus one-hot encoded Gender and Store Location (if added during training)\n",
        "\n",
        "store_id = st.selectbox(\"Store ID\", unique_store_ids)\n",
        "store_location = st.selectbox(\"Store Location\", unique_store_locations)\n",
        "product_id = st.selectbox(\"Product ID\", unique_product_ids)\n",
        "\n",
        "transaction_qty = st.number_input(\"Transaction Quantity\", min_value=1, value=1, step=1)\n",
        "unit_price = st.number_input(\"Unit Price\", min_value=0, value=int(original_df['unit_price'].median()), step=1000) # Use median from data, cast to int\n",
        "age = st.number_input(\"Age\", min_value=0, max_value=120, value=int(original_df['Age'].mean()), step=1) # Use mean from data\n",
        "income = st.number_input(\"Income\", min_value=0, value=int(original_df['Income'].median()), step=100000) # Use median from data\n",
        "\n",
        "gender = st.selectbox(\"Gender\", unique_genders)\n",
        "season = st.selectbox(\"Season\", unique_seasons)\n",
        "occupation = st.selectbox(\"Occupation\", unique_occupations)\n",
        "\n",
        "month = st.number_input(\"Month (1-12)\", min_value=1, max_value=12, value=6, step=1)\n",
        "year = st.number_input(\"Year\", min_value=2022, max_value=2030, value=2024, step=1)\n",
        "\n",
        "\n",
        "# Button to trigger prediction\n",
        "predict_button = st.button(\"Predict Demand\")\n",
        "\n",
        "if predict_button:\n",
        "    # Create a DataFrame from user inputs\n",
        "    input_data = {\n",
        "        'store_id': store_id,\n",
        "        'store_location': store_location,\n",
        "        'product_id': product_id,\n",
        "        'transaction_qty': transaction_qty,\n",
        "        'unit_price': unit_price,\n",
        "        'Age': age,\n",
        "        'Income': income,\n",
        "        'Gender': gender,\n",
        "        'month': month,\n",
        "        'year': year,\n",
        "        'Season': season,\n",
        "        'Occupation': occupation\n",
        "    }\n",
        "    input_df = pd.DataFrame([input_data])\n",
        "\n",
        "    # Apply one-hot encoding to categorical columns in input_df to match expected_cols\n",
        "    # We need to apply one-hot encoding to 'Season', 'Occupation', 'Gender', and 'store_location'\n",
        "\n",
        "    categorical_cols_to_encode = ['Season', 'Occupation', 'Gender', 'store_location'] # Add 'store_location'\n",
        "\n",
        "    # Apply one-hot encoding to the input DataFrame\n",
        "    # Use the unique values from the original data to ensure all possible categories are covered\n",
        "    input_df = pd.get_dummies(input_df, columns=['Season'], prefix='Season', drop_first=True)\n",
        "    input_df = pd.get_dummies(input_df, columns=['Occupation'], prefix='Occupation', drop_first=True)\n",
        "    input_df = pd.get_dummies(input_df, columns=['Gender'], prefix='Gender', drop_first=True)\n",
        "    input_df = pd.get_dummies(input_df, columns=['store_location'], prefix='store_location', drop_first=True)\n",
        "\n",
        "\n",
        "    # Reindex the input_df to match the expected columns from training and fill missing columns with 0\n",
        "    # This step is crucial to ensure the input data has the same columns in the same order as X_train\n",
        "    # expected_cols is loaded from 'xgboost_features.pkl'\n",
        "\n",
        "    input_df = input_df.reindex(columns=expected_cols, fill_value=0)\n",
        "\n",
        "    # Make prediction\n",
        "    try:\n",
        "        predicted_demand = model.predict(input_df)\n",
        "        st.success(f\"Predicted Product Demand: {predicted_demand[0]:,.2f} VND\")\n",
        "    except Exception as e:\n",
        "        st.error(f\"An error occurred during prediction: {e}\")\n",
        "\n",
        "st.write(\"To run this app, save the code as `app.py` and run `streamlit run app.py` in your terminal or use the ngrok tunnel.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Build a Streamlit application that takes \"store_location\", \"Gender\", \"Age\", \"Total_Bill\", \"Season\", \"Size\", \"transaction_qty\" as input and uses the trained model to predict  suggested product category\n",
        "\n",
        "# Save the trained Gradient Boosting model\n",
        "model_path_gb = 'gradient_boosting_model.pkl'\n",
        "joblib.dump(gb_model, model_path_gb)\n",
        "print(f\"\\nGradient Boosting model saved to {model_path_gb}\")\n",
        "\n",
        "# Save the LabelEncoder used for the target variable (product_category)\n",
        "label_encoder_target_path = 'product_category_label_encoder.pkl'\n",
        "joblib.dump(label_encoder_y, label_encoder_target_path) # Use the label_encoder_y defined earlier for 'product_category'\n",
        "print(f\"Target LabelEncoder saved to {label_encoder_target_path}\")\n",
        "\n",
        "# Save the list of feature columns used for training the GB model\n",
        "# The GB model used the original 'features' list after applying LabelEncoding\n",
        "features_path_gb = 'gradient_boosting_features.pkl'\n",
        "joblib.dump(features, features_path_gb)\n",
        "print(f\"Feature columns saved to {features_path_gb}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ALU2pCzUlcZ",
        "outputId": "1da61ea3-67cf-40a2-8ac3-efe4fdc008c7"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Gradient Boosting model saved to gradient_boosting_model.pkl\n",
            "Target LabelEncoder saved to product_category_label_encoder.pkl\n",
            "Feature columns saved to gradient_boosting_features.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: save the code as `app.py`\n",
        "\n",
        "%%writefile app.py\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import numpy as np\n",
        "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
        "import xgboost as xgb\n",
        "from scipy.stats import uniform, randint\n",
        "from sklearn.preprocessing import LabelEncoder # Import LabelEncoder\n",
        "import streamlit as st\n",
        "import joblib\n",
        "import os # Import os for checking file existence\n",
        "\n",
        "# Mount Google Drive (necessary for Colab execution, remove for local Streamlit)\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# --- File Paths ---\n",
        "# Update these paths to reflect the correct location of your data and saved files\n",
        "file_path_data = '/content/drive/MyDrive/HK II | 24.25/AI in Biz /BTL /data.csv' # Path to your training data\n",
        "# file_path_data = '/content/drive/MyDrive/Phú Quốc/du_lieu_da_them_season_va_income( dữ liệu cuối cùng).xlsx - Sheet1.csv' # Path to your data for unique values\n",
        "model_path = 'xgboost_model.pkl'\n",
        "features_path = 'xgboost_features.pkl'\n",
        "label_encoder_target_path = 'product_category_label_encoder.pkl'\n",
        "\n",
        "# --- Load Data and Setup ---\n",
        "# Load the original dataframe to get unique values for dropdowns\n",
        "# This is needed to populate selectboxes with actual data values\n",
        "try:\n",
        "    original_df = pd.read_csv(file_path_data)\n",
        "    # Extract unique values for dropdowns. Adjust column names as per your data.\n",
        "    # Ensure these column names match the features used in the model.\n",
        "    unique_store_locations = sorted(original_df['store_location'].dropna().unique().tolist())\n",
        "    unique_seasons = sorted(original_df['Season'].dropna().unique().tolist())\n",
        "    unique_genders = sorted(original_df['Gender'].dropna().unique().tolist())\n",
        "    # Add other unique columns relevant to your features here\n",
        "    # Example: unique_occupations = sorted(original_df['Occupation'].dropna().unique().tolist())\n",
        "    # Example: unique_product_categories = sorted(original_df['product_category'].dropna().unique().tolist()) # For target decoding later if needed\n",
        "\n",
        "except FileNotFoundError:\n",
        "    st.error(f\"Original data file not found at {file_path_data}. Please ensure the file exists.\")\n",
        "    st.stop()\n",
        "except KeyError as e:\n",
        "    st.error(f\"Missing expected column in original data file: {e}\")\n",
        "    st.stop()\n",
        "except Exception as e:\n",
        "    st.error(f\"Error loading original data: {e}\")\n",
        "    st.stop()\n",
        "\n",
        "# --- Load Trained Model and Artifacts ---\n",
        "# Load the trained XGBoost model\n",
        "if not os.path.exists(model_path):\n",
        "    st.error(f\"Model file '{model_path}' not found. Please train and save the model first.\")\n",
        "    st.stop()\n",
        "try:\n",
        "    model = joblib.load(model_path)\n",
        "except Exception as e:\n",
        "    st.error(f\"Error loading the model file: {e}\")\n",
        "    st.stop()\n",
        "\n",
        "# Load the feature columns used during training\n",
        "if not os.path.exists(features_path):\n",
        "    st.error(f\"Feature columns file '{features_path}' not found. Please run the training steps first to save X_train columns.\")\n",
        "    st.stop()\n",
        "try:\n",
        "    expected_cols = joblib.load(features_path)\n",
        "except Exception as e:\n",
        "    st.error(f\"Error loading the feature columns file: {e}\")\n",
        "    st.stop()\n",
        "\n",
        "# Load the LabelEncoder for the target variable if you need to decode predictions\n",
        "# if not os.path.exists(label_encoder_target_path):\n",
        "#     st.warning(f\"Target LabelEncoder file '{label_encoder_target_path}' not found. Predictions will be numerical.\")\n",
        "#     label_encoder_y = None\n",
        "# else:\n",
        "#     try:\n",
        "#         label_encoder_y = joblib.load(label_encoder_target_path)\n",
        "#     except Exception as e:\n",
        "#         st.error(f\"Error loading the Target LabelEncoder file: {e}\")\n",
        "#         label_encoder_y = None\n",
        "\n",
        "\n",
        "# --- Streamlit App Layout ---\n",
        "st.title(\"Seasonal Product Demand Forecasting\")\n",
        "st.header(\"Input the details to predict product demand for a season.\")\n",
        "\n",
        "# --- Input Widgets ---\n",
        "# Define input widgets based on the features used for training (expected_cols)\n",
        "# You need to map the feature columns in expected_cols back to meaningful user inputs.\n",
        "# For one-hot encoded features like Season_Spring, you'll need a single 'Season' selectbox.\n",
        "\n",
        "# Example based on the features you mentioned: [\"store_location\", \"Gender\", \"Age\", \"Total_Bill\", \"Season\", \"Size\", \"transaction_qty\"]\n",
        "# And the ones used for XGBoost: derived from \"store_location\", \"Gender\", \"Age\", \"Total_Bill\", \"Season\", \"Size\", \"transaction_qty\" after one-hot encoding\n",
        "\n",
        "st.subheader(\"Enter Product and Customer Details:\")\n",
        "\n",
        "# Use dropdowns for categorical features\n",
        "store_location = st.selectbox(\"Store Location\", unique_store_locations)\n",
        "gender = st.selectbox(\"Gender\", unique_genders)\n",
        "season = st.selectbox(\"Season\", unique_seasons)\n",
        "# Assuming 'Size' is also a categorical feature based on your previous code snippet\n",
        "# You would need unique values for 'Size' from your original data\n",
        "# unique_sizes = sorted(original_df['Size'].dropna().unique().tolist())\n",
        "# size = st.selectbox(\"Size\", unique_sizes)\n",
        "\n",
        "\n",
        "# Use number inputs for numerical features\n",
        "age = st.number_input(\"Age\", min_value=0, max_value=120, value=30, step=1) # Default to a reasonable value\n",
        "total_bill = st.number_input(\"Total Bill\", min_value=0.0, value=100.0, step=0.1) # Default to a reasonable value\n",
        "transaction_qty = st.number_input(\"Transaction Quantity\", min_value=1, value=1, step=1)\n",
        "\n",
        "\n",
        "# Button to trigger prediction\n",
        "predict_button = st.button(\"Predict Demand\")\n",
        "\n",
        "# --- Prediction Logic ---\n",
        "if predict_button:\n",
        "    # Create a DataFrame from user inputs\n",
        "    input_data = {\n",
        "        'Age': age,\n",
        "        'Total_Bill': total_bill,\n",
        "        'transaction_qty': transaction_qty,\n",
        "        'Gender': gender,\n",
        "        'Season': season,\n",
        "        # 'Size': size, # Include if 'Size' is a feature\n",
        "        'store_location': store_location,\n",
        "        # Add other numerical features here if used in your model training\n",
        "        # e.g., 'Income', 'unit_price', 'month', 'year'\n",
        "    }\n",
        "    input_df = pd.DataFrame([input_data])\n",
        "\n",
        "    # Apply one-hot encoding to the categorical columns in the input DataFrame\n",
        "    # This must match the encoding applied to the training data (X_train)\n",
        "    categorical_cols_to_encode_input = ['Gender', 'Season', 'store_location'] # Add 'Size' if used\n",
        "    # Ensure the column names match your input_df keys\n",
        "\n",
        "    # Apply one-hot encoding. This requires knowing the unique categories seen during training.\n",
        "    # A safer way is to use the original_df to get all possible categories.\n",
        "    # This ensures that even if a category is not in the current input, the column exists with value 0.\n",
        "\n",
        "    input_df = pd.get_dummies(input_df, columns=['Gender'], prefix='Gender', drop_first=False) # Keep all dummies for reindexing later\n",
        "    input_df = pd.get_dummies(input_df, columns=['Season'], prefix='Season', drop_first=False) # Keep all dummies\n",
        "    input_df = pd.get_dummies(input_df, columns=['store_location'], prefix='store_location', drop_first=False) # Keep all dummies\n",
        "    # input_df = pd.get_dummies(input_df, columns=['Size'], prefix='Size', drop_first=False) # Include if Size is used\n",
        "\n",
        "    # Reindex the input_df to match the expected columns from training (expected_cols)\n",
        "    # Fill missing columns (due to one-hot encoding not generating all possible columns in the input) with 0\n",
        "    input_df_processed = input_df.reindex(columns=expected_cols, fill_value=0)\n",
        "\n",
        "    # Ensure the order of columns matches the training data\n",
        "    input_df_processed = input_df_processed[expected_cols]\n",
        "\n",
        "\n",
        "    # Make prediction\n",
        "    try:\n",
        "        # Predict the numerical product category label\n",
        "        predicted_label = model.predict(input_df_processed)[0]\n",
        "\n",
        "        # Decode the predicted label back to the original product category string\n",
        "        # This requires the LabelEncoder used during training\n",
        "        # If label_encoder_y is None, we can only display the numerical label\n",
        "        # if label_encoder_y:\n",
        "        #     predicted_category = label_encoder_y.inverse_transform([int(predicted_label)])[0]\n",
        "        #     st.success(f\"Predicted Product Category: **{predicted_category}**\")\n",
        "        # else:\n",
        "        st.success(f\"Predicted Product Category (Numerical Label): **{int(predicted_label)}**\") # Display as integer\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        st.error(f\"An error occurred during prediction: {e}\")\n",
        "        st.write(\"Please check the input values and ensure the model and feature files are correctly loaded.\")\n",
        "\n",
        "st.write(\"To run this app:\")\n",
        "st.code(\"1. Save the code above as `app.py`\")\n",
        "st.code(\"2. Ensure 'xgboost_model.pkl' and 'xgboost_features.pkl' are in the same directory or accessible path.\")\n",
        "st.code(\"3. Open your terminal or command prompt.\")\n",
        "st.code(\"4. Navigate to the directory where you saved `app.py`.\")\n",
        "st.code(\"5. Run the command: `streamlit run app.py`\")\n",
        "st.write(\"If running on Google Colab, you'll need to use ngrok or another tunneling service to expose the Streamlit app.\")\n",
        "\n",
        "# Note: The training steps (Gradient Boosting and XGBoost with RandomizedSearchCV)\n",
        "# are still included in this file. When running `streamlit run app.py` locally,\n",
        "# these training steps will execute every time the app starts, which is inefficient.\n",
        "# It's recommended to perform training and model saving in a separate script or notebook,\n",
        "# and only load the saved model and artifacts in the Streamlit app.\n",
        "\n",
        "# Example of saving the trained XGBoost model and feature columns after training (in your training script/notebook):\n",
        "# Assuming `random_search` has been fitted and `expected_cols` are the columns of X_train\n",
        "\n",
        "# # Save the best XGBoost model\n",
        "# best_xgb_model = random_search.best_estimator_\n",
        "# joblib.dump(best_xgb_model, model_path)\n",
        "# print(f\"\\nBest XGBoost model saved to {model_path}\")\n",
        "\n",
        "# # Save the list of feature columns\n",
        "# joblib.dump(X_train.columns.tolist(), features_path)\n",
        "# print(f\"Feature columns list saved to {features_path}\")\n",
        "\n",
        "# # Save the target LabelEncoder\n",
        "# joblib.dump(label_encoder_y, label_encoder_target_path)\n",
        "# print(f\"Target LabelEncoder saved to {label_encoder_target_path}\")\n",
        "\n",
        "```"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4YQAD-G5c68C",
        "outputId": "284df4fb-03f9-45d9-fd32-f2ef1f0cce81"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}